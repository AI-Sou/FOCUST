# -*- coding: utf-8 -*-
"""
GPUå†…å­˜ä¼˜åŒ–å·¥å…·
ç¡®ä¿GPUåˆ‡æ¢æ—¶å†…å­˜å¾—åˆ°å½»åº•æ¸…ç†å’Œæ­£ç¡®åˆ†é…

ä¸»è¦åŠŸèƒ½ï¼š
1. æ™ºèƒ½å†…å­˜æ¸…ç†ç­–ç•¥
2. å†…å­˜ç¢ç‰‡æ•´ç†
3. é¢„åˆ†é…å†…å­˜æ± ç®¡ç†  
4. å†…å­˜ä½¿ç”¨ç›‘æ§å’ŒæŠ¥å‘Š
5. è‡ªåŠ¨å†…å­˜ä¼˜åŒ–å»ºè®®
"""

import os
import sys
import gc
import time
import threading
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum

import torch
import torch.nn as nn

try:
    import pynvml
    PYNVML_AVAILABLE = True
except ImportError:
    PYNVML_AVAILABLE = False


class MemoryCleanupLevel(Enum):
    """å†…å­˜æ¸…ç†çº§åˆ«"""
    BASIC = "basic"          # åŸºç¡€æ¸…ç†
    STANDARD = "standard"    # æ ‡å‡†æ¸…ç†  
    AGGRESSIVE = "aggressive" # æ¿€è¿›æ¸…ç†
    DEEP = "deep"           # æ·±åº¦æ¸…ç†


@dataclass
class MemoryUsageReport:
    """å†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
    device_id: str
    total_mb: float
    allocated_mb: float
    reserved_mb: float
    free_mb: float
    fragmented_mb: float
    usage_percent: float
    efficiency_percent: float
    recommendations: List[str]


class GPUMemoryOptimizer:
    """
    GPUå†…å­˜ä¼˜åŒ–å™¨
    
    ç‰¹æ€§ï¼š
    - å¤šçº§å†…å­˜æ¸…ç†ç­–ç•¥
    - å†…å­˜ç¢ç‰‡æ£€æµ‹å’Œæ•´ç†
    - æ™ºèƒ½é¢„åˆ†é…ç®¡ç†
    - å®æ—¶å†…å­˜ç›‘æ§
    - ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
    """
    
    def __init__(self):
        self.logger = self._setup_logger()
        self._memory_pools: Dict[str, List[torch.Tensor]] = {}
        self._cleanup_callbacks: List[callable] = []
        self._monitoring_active = False
        self._monitor_thread = None
        
        # åˆå§‹åŒ–NVML
        if PYNVML_AVAILABLE:
            try:
                pynvml.nvmlInit()
                self.nvml_available = True
            except Exception as e:
                self.logger.warning(f"NVMLåˆå§‹åŒ–å¤±è´¥: {e}")
                self.nvml_available = False
        else:
            self.nvml_available = False
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger("GPUMemoryOptimizer")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [å†…å­˜ä¼˜åŒ–å™¨] %(levelname)s: %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def cleanup_device_memory(self, device_id: str, 
                            level: MemoryCleanupLevel = MemoryCleanupLevel.STANDARD,
                            force: bool = False) -> bool:
        """
        æ¸…ç†æŒ‡å®šè®¾å¤‡çš„å†…å­˜
        
        Args:
            device_id: è®¾å¤‡ID (å¦‚ 'cuda:0', 'cuda:1', 'cpu')
            level: æ¸…ç†çº§åˆ«
            force: æ˜¯å¦å¼ºåˆ¶æ¸…ç†ï¼ˆå¯èƒ½å½±å“æ­£åœ¨è¿è¡Œçš„æ¨¡å‹ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸæ¸…ç†
        """
        self.logger.info(f"ğŸ§¹ å¼€å§‹æ¸…ç†è®¾å¤‡ {device_id} å†…å­˜ (çº§åˆ«: {level.value})")
        
        try:
            if device_id == "cpu":
                return self._cleanup_cpu_memory(level, force)
            elif device_id.startswith("cuda:"):
                return self._cleanup_gpu_memory(device_id, level, force)
            else:
                self.logger.error(f"ä¸æ”¯æŒçš„è®¾å¤‡ç±»å‹: {device_id}")
                return False
                
        except Exception as e:
            self.logger.error(f"æ¸…ç†è®¾å¤‡ {device_id} å†…å­˜æ—¶å‡ºé”™: {e}")
            return False
    
    def _cleanup_cpu_memory(self, level: MemoryCleanupLevel, force: bool) -> bool:
        """æ¸…ç†CPUå†…å­˜"""
        self.logger.info("ğŸ§¹ æ¸…ç†CPUå†…å­˜")
        
        # æ‰§è¡Œåƒåœ¾å›æ”¶
        if level in [MemoryCleanupLevel.BASIC, MemoryCleanupLevel.STANDARD]:
            gc.collect()
        elif level in [MemoryCleanupLevel.AGGRESSIVE, MemoryCleanupLevel.DEEP]:
            # å¤šè½®åƒåœ¾å›æ”¶
            for i in range(3):
                collected = gc.collect()
                self.logger.debug(f"åƒåœ¾å›æ”¶ç¬¬{i+1}è½®: å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
                if i < 2:  # å‰ä¸¤è½®ä¹‹é—´çŸ­æš‚åœé¡¿
                    time.sleep(0.1)
        
        # æ·±åº¦æ¸…ç†ï¼šæ‰‹åŠ¨è°ƒç”¨æ‰€æœ‰å·²æ³¨å†Œçš„æ¸…ç†å›è°ƒ
        if level == MemoryCleanupLevel.DEEP and force:
            for callback in self._cleanup_callbacks:
                try:
                    callback()
                except Exception as e:
                    self.logger.warning(f"æ‰§è¡Œæ¸…ç†å›è°ƒå¤±è´¥: {e}")
        
        self.logger.info("âœ… CPUå†…å­˜æ¸…ç†å®Œæˆ")
        return True
    
    def _cleanup_gpu_memory(self, device_id: str, level: MemoryCleanupLevel, force: bool) -> bool:
        """æ¸…ç†GPUå†…å­˜"""
        if not torch.cuda.is_available():
            self.logger.warning("CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå†…å­˜æ¸…ç†")
            return False
        
        try:
            gpu_index = int(device_id.split(':')[1])
            if gpu_index >= torch.cuda.device_count():
                self.logger.error(f"GPUç´¢å¼• {gpu_index} è¶…å‡ºèŒƒå›´")
                return False
        except (ValueError, IndexError):
            self.logger.error(f"æ— æ•ˆçš„GPUè®¾å¤‡ID: {device_id}")
            return False
        
        self.logger.info(f"ğŸ§¹ æ¸…ç†GPU {gpu_index} å†…å­˜ (çº§åˆ«: {level.value})")
        
        # ä¿å­˜å½“å‰è®¾å¤‡
        original_device = None
        try:
            original_device = torch.cuda.current_device()
        except:
            pass
        
        try:
            # åˆ‡æ¢åˆ°ç›®æ ‡GPU
            torch.cuda.set_device(gpu_index)
            
            # è®°å½•æ¸…ç†å‰çš„å†…å­˜çŠ¶æ€
            before_allocated = torch.cuda.memory_allocated(gpu_index) / (1024**2)
            before_reserved = torch.cuda.memory_reserved(gpu_index) / (1024**2)
            
            # åŸºç¡€æ¸…ç†
            if level == MemoryCleanupLevel.BASIC:
                torch.cuda.empty_cache()
            
            # æ ‡å‡†æ¸…ç†  
            elif level == MemoryCleanupLevel.STANDARD:
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                gc.collect()
            
            # æ¿€è¿›æ¸…ç†
            elif level == MemoryCleanupLevel.AGGRESSIVE:
                # å¤šè½®æ¸…ç†
                for i in range(2):
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
                    gc.collect()
                    time.sleep(0.05)
                
                # é‡ç½®å†…å­˜ç»Ÿè®¡
                torch.cuda.reset_peak_memory_stats(gpu_index)
                torch.cuda.reset_accumulated_memory_stats(gpu_index)
            
            # æ·±åº¦æ¸…ç†
            elif level == MemoryCleanupLevel.DEEP:
                # æ¸…ç†å†…å­˜æ± 
                if device_id in self._memory_pools:
                    pool_tensors = self._memory_pools[device_id]
                    for tensor in pool_tensors:
                        try:
                            del tensor
                        except:
                            pass
                    self._memory_pools[device_id].clear()
                    self.logger.info(f"æ¸…ç†äº† {len(pool_tensors)} ä¸ªé¢„åˆ†é…å¼ é‡")
                
                # å¼ºåˆ¶åŒæ­¥å’Œå¤šè½®æ¸…ç†
                for i in range(3):
                    torch.cuda.synchronize(gpu_index)
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
                    gc.collect()
                    time.sleep(0.1)
                
                # é‡ç½®æ‰€æœ‰å†…å­˜ç»Ÿè®¡
                torch.cuda.reset_peak_memory_stats(gpu_index)
                torch.cuda.reset_accumulated_memory_stats(gpu_index)
                
                # å¦‚æœå¼ºåˆ¶æ¸…ç†ï¼Œæ‰§è¡Œé¢å¤–çš„æ¸…ç†æ“ä½œ
                if force:
                    # æ‰§è¡Œæ³¨å†Œçš„æ¸…ç†å›è°ƒ
                    for callback in self._cleanup_callbacks:
                        try:
                            callback()
                        except Exception as e:
                            self.logger.warning(f"æ‰§è¡Œæ¸…ç†å›è°ƒå¤±è´¥: {e}")
                    
                    # å°è¯•é‡æ–°åˆ†é…å’Œç«‹å³é‡Šæ”¾ä¸€ä¸ªå¤§å¼ é‡æ¥æ•´ç†å†…å­˜
                    try:
                        # è·å–å¯ç”¨å†…å­˜å¤§å°
                        free_memory = torch.cuda.mem_get_info(gpu_index)[0]
                        # åˆ†é…80%çš„å¯ç”¨å†…å­˜æ¥è§¦å‘å†…å­˜æ•´ç†
                        size_to_alloc = int(free_memory * 0.8 / 4)  # float32 = 4 bytes
                        if size_to_alloc > 0:
                            temp_tensor = torch.empty(size_to_alloc, dtype=torch.float32, device=device_id)
                            del temp_tensor
                            torch.cuda.empty_cache()
                            self.logger.info("æ‰§è¡Œäº†å†…å­˜ç¢ç‰‡æ•´ç†")
                    except Exception as e:
                        self.logger.debug(f"å†…å­˜ç¢ç‰‡æ•´ç†å¤±è´¥: {e}")
            
            # æœ€ç»ˆåŒæ­¥
            torch.cuda.synchronize(gpu_index)
            
            # è®°å½•æ¸…ç†åçš„å†…å­˜çŠ¶æ€
            after_allocated = torch.cuda.memory_allocated(gpu_index) / (1024**2)
            after_reserved = torch.cuda.memory_reserved(gpu_index) / (1024**2)
            
            freed_allocated = before_allocated - after_allocated
            freed_reserved = before_reserved - after_reserved
            
            self.logger.info(f"âœ… GPU {gpu_index} å†…å­˜æ¸…ç†å®Œæˆ:")
            self.logger.info(f"   é‡Šæ”¾å·²åˆ†é…å†…å­˜: {freed_allocated:.1f} MB")
            self.logger.info(f"   é‡Šæ”¾å·²ä¿ç•™å†…å­˜: {freed_reserved:.1f} MB")
            self.logger.info(f"   å½“å‰å·²åˆ†é…: {after_allocated:.1f} MB")
            self.logger.info(f"   å½“å‰å·²ä¿ç•™: {after_reserved:.1f} MB")
            
            return True
            
        finally:
            # æ¢å¤åŸè®¾å¤‡
            if original_device is not None and original_device != gpu_index:
                try:
                    torch.cuda.set_device(original_device)
                except:
                    pass
    
    def preallocate_memory_pool(self, device_id: str, pool_size_mb: int) -> bool:
        """
        ä¸ºè®¾å¤‡é¢„åˆ†é…å†…å­˜æ± 
        
        Args:
            device_id: è®¾å¤‡ID
            pool_size_mb: å†…å­˜æ± å¤§å°(MB)
            
        Returns:
            æ˜¯å¦æˆåŠŸé¢„åˆ†é…
        """
        if not device_id.startswith("cuda:"):
            self.logger.warning("å†…å­˜æ± ä»…æ”¯æŒGPUè®¾å¤‡")
            return False
        
        if not torch.cuda.is_available():
            self.logger.warning("CUDAä¸å¯ç”¨")
            return False
        
        try:
            gpu_index = int(device_id.split(':')[1])
            if gpu_index >= torch.cuda.device_count():
                return False
        except (ValueError, IndexError):
            return False
        
        self.logger.info(f"ğŸ“¦ ä¸ºè®¾å¤‡ {device_id} é¢„åˆ†é… {pool_size_mb} MB å†…å­˜æ± ")
        
        try:
            # åˆ‡æ¢åˆ°ç›®æ ‡è®¾å¤‡
            original_device = torch.cuda.current_device()
            torch.cuda.set_device(gpu_index)
            
            # è®¡ç®—éœ€è¦åˆ†é…çš„å¼ é‡æ•°é‡å’Œå¤§å°
            # ä½¿ç”¨å¤šä¸ªä¸­ç­‰å¤§å°çš„å¼ é‡è€Œä¸æ˜¯ä¸€ä¸ªå¤§å¼ é‡ï¼Œä»¥å‡å°‘ç¢ç‰‡
            chunk_size_mb = min(pool_size_mb, 256)  # æ¯ä¸ªå—æœ€å¤§256MB
            num_chunks = pool_size_mb // chunk_size_mb
            remaining_mb = pool_size_mb % chunk_size_mb
            
            # æ¸…ç†æ—§çš„å†…å­˜æ± 
            if device_id in self._memory_pools:
                for tensor in self._memory_pools[device_id]:
                    del tensor
                self._memory_pools[device_id].clear()
            else:
                self._memory_pools[device_id] = []
            
            # åˆ†é…å†…å­˜å—
            total_allocated = 0
            for i in range(num_chunks):
                chunk_elements = int(chunk_size_mb * 1024 * 1024 / 4)  # float32 = 4 bytes
                tensor = torch.empty(chunk_elements, dtype=torch.float32, device=device_id)
                self._memory_pools[device_id].append(tensor)
                total_allocated += chunk_size_mb
                self.logger.debug(f"åˆ†é…å†…å­˜å— {i+1}/{num_chunks}: {chunk_size_mb} MB")
            
            # åˆ†é…å‰©ä½™å†…å­˜
            if remaining_mb > 0:
                remaining_elements = int(remaining_mb * 1024 * 1024 / 4)
                tensor = torch.empty(remaining_elements, dtype=torch.float32, device=device_id)
                self._memory_pools[device_id].append(tensor)
                total_allocated += remaining_mb
                self.logger.debug(f"åˆ†é…å‰©ä½™å†…å­˜: {remaining_mb} MB")
            
            # æ¢å¤åŸè®¾å¤‡
            torch.cuda.set_device(original_device)
            
            self.logger.info(f"âœ… æˆåŠŸé¢„åˆ†é… {total_allocated} MB å†…å­˜æ± ")
            return True
            
        except Exception as e:
            self.logger.error(f"é¢„åˆ†é…å†…å­˜æ± å¤±è´¥: {e}")
            return False
    
    def get_memory_usage_report(self, device_id: str) -> MemoryUsageReport:
        """ç”Ÿæˆå†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
        try:
            if device_id == "cpu":
                return self._get_cpu_memory_report()
            elif device_id.startswith("cuda:"):
                return self._get_gpu_memory_report(device_id)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è®¾å¤‡: {device_id}")
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå†…å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            return MemoryUsageReport(
                device_id=device_id,
                total_mb=0, allocated_mb=0, reserved_mb=0, free_mb=0,
                fragmented_mb=0, usage_percent=0, efficiency_percent=0,
                recommendations=[f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {str(e)}"]
            )
    
    def _get_cpu_memory_report(self) -> MemoryUsageReport:
        """ç”ŸæˆCPUå†…å­˜æŠ¥å‘Š"""
        try:
            import psutil
            memory = psutil.virtual_memory()
            
            total_mb = memory.total / (1024**2)
            used_mb = memory.used / (1024**2)
            free_mb = memory.available / (1024**2)
            usage_percent = memory.percent
            
            # ç”Ÿæˆå»ºè®®
            recommendations = []
            if usage_percent > 90:
                recommendations.append("CPUå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å…³é—­ä¸å¿…è¦çš„åº”ç”¨ç¨‹åº")
            elif usage_percent > 80:
                recommendations.append("CPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")
            
            return MemoryUsageReport(
                device_id="cpu",
                total_mb=total_mb,
                allocated_mb=used_mb,
                reserved_mb=used_mb,
                free_mb=free_mb,
                fragmented_mb=0,  # CPUå†…å­˜ç¢ç‰‡éš¾ä»¥å‡†ç¡®è®¡ç®—
                usage_percent=usage_percent,
                efficiency_percent=100,  # CPUå†…å­˜æ•ˆç‡é€šå¸¸è¾ƒé«˜
                recommendations=recommendations
            )
        except Exception as e:
            raise Exception(f"è·å–CPUå†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
    
    def _get_gpu_memory_report(self, device_id: str) -> MemoryUsageReport:
        """ç”ŸæˆGPUå†…å­˜æŠ¥å‘Š"""
        if not torch.cuda.is_available():
            raise Exception("CUDAä¸å¯ç”¨")
        
        try:
            gpu_index = int(device_id.split(':')[1])
            if gpu_index >= torch.cuda.device_count():
                raise Exception(f"GPUç´¢å¼•è¶…å‡ºèŒƒå›´: {gpu_index}")
        except (ValueError, IndexError):
            raise Exception(f"æ— æ•ˆçš„GPUè®¾å¤‡ID: {device_id}")
        
        try:
            # PyTorchå†…å­˜ä¿¡æ¯
            allocated_mb = torch.cuda.memory_allocated(gpu_index) / (1024**2)
            reserved_mb = torch.cuda.memory_reserved(gpu_index) / (1024**2)
            
            # æ€»å†…å­˜å’Œå¯ç”¨å†…å­˜
            if self.nvml_available:
                try:
                    handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)
                    mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                    total_mb = mem_info.total / (1024**2)
                    nvml_used_mb = mem_info.used / (1024**2)
                    nvml_free_mb = mem_info.free / (1024**2)
                except Exception:
                    props = torch.cuda.get_device_properties(gpu_index)
                    total_mb = props.total_memory / (1024**2)
                    nvml_used_mb = allocated_mb
                    nvml_free_mb = total_mb - nvml_used_mb
            else:
                props = torch.cuda.get_device_properties(gpu_index)
                total_mb = props.total_memory / (1024**2)
                nvml_used_mb = allocated_mb
                nvml_free_mb = total_mb - nvml_used_mb
            
            # è®¡ç®—å†…å­˜ç¢ç‰‡
            fragmented_mb = reserved_mb - allocated_mb
            
            # è®¡ç®—ä½¿ç”¨ç‡å’Œæ•ˆç‡
            usage_percent = (nvml_used_mb / total_mb) * 100 if total_mb > 0 else 0
            efficiency_percent = (allocated_mb / reserved_mb) * 100 if reserved_mb > 0 else 100
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = []
            
            if usage_percent > 95:
                recommendations.append("GPUå†…å­˜ä¸¥é‡ä¸è¶³ï¼Œå»ºè®®å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–ä½¿ç”¨å†…å­˜ä¼˜åŒ–ç­–ç•¥")
            elif usage_percent > 85:
                recommendations.append("GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå»ºè®®ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")
            
            if fragmented_mb > 1024:  # è¶…è¿‡1GBç¢ç‰‡
                recommendations.append("æ£€æµ‹åˆ°å¤§é‡å†…å­˜ç¢ç‰‡ï¼Œå»ºè®®æ‰§è¡Œæ·±åº¦å†…å­˜æ¸…ç†")
            elif fragmented_mb > 512:  # è¶…è¿‡512MBç¢ç‰‡
                recommendations.append("å­˜åœ¨ä¸€å®šå†…å­˜ç¢ç‰‡ï¼Œå»ºè®®å®šæœŸæ¸…ç†GPUå†…å­˜")
            
            if efficiency_percent < 70:
                recommendations.append("å†…å­˜ä½¿ç”¨æ•ˆç‡è¾ƒä½ï¼Œå­˜åœ¨è¾ƒå¤šæœªä½¿ç”¨çš„ä¿ç•™å†…å­˜")
            
            if not recommendations:
                recommendations.append("å†…å­˜ä½¿ç”¨çŠ¶å†µè‰¯å¥½")
            
            return MemoryUsageReport(
                device_id=device_id,
                total_mb=total_mb,
                allocated_mb=allocated_mb,
                reserved_mb=reserved_mb,
                free_mb=nvml_free_mb,
                fragmented_mb=fragmented_mb,
                usage_percent=usage_percent,
                efficiency_percent=efficiency_percent,
                recommendations=recommendations
            )
            
        except Exception as e:
            raise Exception(f"è·å–GPUå†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
    
    def optimize_memory_for_training(self, device_id: str, model_size_mb: float, 
                                   batch_size: int) -> Dict[str, Any]:
        """
        ä¸ºè®­ç»ƒä¼˜åŒ–å†…å­˜é…ç½®
        
        Args:
            device_id: è®¾å¤‡ID
            model_size_mb: é¢„ä¼°æ¨¡å‹å¤§å°(MB)
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            ä¼˜åŒ–å»ºè®®å’Œé…ç½®
        """
        self.logger.info(f"ğŸ”§ ä¸ºè®¾å¤‡ {device_id} ä¼˜åŒ–è®­ç»ƒå†…å­˜é…ç½®")
        
        try:
            report = self.get_memory_usage_report(device_id)
            
            # ä¼°ç®—è®­ç»ƒæ‰€éœ€å†…å­˜
            # æ¨¡å‹å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ¿€æ´»å€¼
            estimated_model_memory = model_size_mb * 4  # å‚æ•°+æ¢¯åº¦+ä¼˜åŒ–å™¨çŠ¶æ€
            estimated_activation_memory = model_size_mb * batch_size * 0.5  # ç²—ç•¥ä¼°ç®—
            total_estimated = estimated_model_memory + estimated_activation_memory
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            optimization = {
                "current_memory": {
                    "total_mb": report.total_mb,
                    "available_mb": report.free_mb,
                    "usage_percent": report.usage_percent
                },
                "estimated_requirements": {
                    "model_memory_mb": estimated_model_memory,
                    "activation_memory_mb": estimated_activation_memory,
                    "total_estimated_mb": total_estimated
                },
                "recommendations": [],
                "suggested_batch_size": batch_size,
                "memory_sufficient": True
            }
            
            # æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³
            required_memory = total_estimated * 1.2  # 20%å®‰å…¨ä½™é‡
            if required_memory > report.free_mb:
                optimization["memory_sufficient"] = False
                optimization["recommendations"].append(
                    f"å†…å­˜ä¸è¶³ï¼šéœ€è¦ {required_memory:.0f} MBï¼Œå¯ç”¨ {report.free_mb:.0f} MB"
                )
                
                # å»ºè®®æ–°çš„æ‰¹æ¬¡å¤§å°
                max_activation_memory = report.free_mb - estimated_model_memory - 1000  # 1GBä½™é‡
                if max_activation_memory > 0:
                    suggested_batch = int((max_activation_memory / (model_size_mb * 0.5)) * batch_size)
                    suggested_batch = max(1, suggested_batch)
                    optimization["suggested_batch_size"] = suggested_batch
                    optimization["recommendations"].append(
                        f"å»ºè®®å°†æ‰¹æ¬¡å¤§å°å‡å°‘åˆ° {suggested_batch}"
                    )
                else:
                    optimization["recommendations"].append("å»ºè®®åˆ‡æ¢åˆ°CPUè®­ç»ƒæˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
            
            # å†…å­˜ä¼˜åŒ–å»ºè®®
            if report.fragmented_mb > 500:
                optimization["recommendations"].append("å»ºè®®å…ˆæ‰§è¡Œå†…å­˜æ¸…ç†ä»¥å‡å°‘ç¢ç‰‡")
            
            if report.efficiency_percent < 80:
                optimization["recommendations"].append("å»ºè®®æ‰§è¡Œæ¿€è¿›å†…å­˜æ¸…ç†ä»¥æé«˜æ•ˆç‡")
            
            return optimization
            
        except Exception as e:
            self.logger.error(f"å†…å­˜ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
            return {
                "error": str(e),
                "memory_sufficient": False,
                "recommendations": ["å†…å­˜åˆ†æå¤±è´¥ï¼Œå»ºè®®ä½¿ç”¨é»˜è®¤é…ç½®"]
            }
    
    def add_cleanup_callback(self, callback: callable):
        """æ·»åŠ å†…å­˜æ¸…ç†å›è°ƒå‡½æ•°"""
        if callback not in self._cleanup_callbacks:
            self._cleanup_callbacks.append(callback)
            self.logger.info("å·²æ·»åŠ å†…å­˜æ¸…ç†å›è°ƒ")
    
    def remove_cleanup_callback(self, callback: callable):
        """ç§»é™¤å†…å­˜æ¸…ç†å›è°ƒå‡½æ•°"""
        if callback in self._cleanup_callbacks:
            self._cleanup_callbacks.remove(callback)
            self.logger.info("å·²ç§»é™¤å†…å­˜æ¸…ç†å›è°ƒ")
    
    def start_memory_monitoring(self, interval_seconds: float = 30.0):
        """å¼€å§‹å†…å­˜ç›‘æ§"""
        if self._monitoring_active:
            self.logger.warning("å†…å­˜ç›‘æ§å·²åœ¨è¿è¡Œ")
            return
        
        self._monitoring_active = True
        self._monitor_thread = threading.Thread(
            target=self._memory_monitor_loop,
            args=(interval_seconds,),
            daemon=True
        )
        self._monitor_thread.start()
        self.logger.info(f"å†…å­˜ç›‘æ§å·²å¯åŠ¨ (é—´éš”: {interval_seconds}s)")
    
    def stop_memory_monitoring(self):
        """åœæ­¢å†…å­˜ç›‘æ§"""
        self._monitoring_active = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=2.0)
        self.logger.info("å†…å­˜ç›‘æ§å·²åœæ­¢")
    
    def _memory_monitor_loop(self, interval_seconds: float):
        """å†…å­˜ç›‘æ§å¾ªç¯"""
        while self._monitoring_active:
            try:
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        device_id = f"cuda:{i}"
                        try:
                            report = self.get_memory_usage_report(device_id)
                            
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨æ¸…ç†
                            if report.usage_percent > 95:
                                self.logger.warning(f"GPU {i} å†…å­˜ä¸¥é‡ä¸è¶³ ({report.usage_percent:.1f}%)")
                                # è‡ªåŠ¨æ‰§è¡Œæ ‡å‡†æ¸…ç†
                                self.cleanup_device_memory(device_id, MemoryCleanupLevel.STANDARD)
                            elif report.fragmented_mb > 1024:
                                self.logger.info(f"GPU {i} å†…å­˜ç¢ç‰‡è¾ƒå¤š ({report.fragmented_mb:.1f} MB)")
                                # è‡ªåŠ¨æ‰§è¡ŒåŸºç¡€æ¸…ç†
                                self.cleanup_device_memory(device_id, MemoryCleanupLevel.BASIC)
                                
                        except Exception as e:
                            self.logger.debug(f"ç›‘æ§GPU {i} å†…å­˜æ—¶å‡ºé”™: {e}")
                
                time.sleep(interval_seconds)
                
            except Exception as e:
                self.logger.error(f"å†…å­˜ç›‘æ§å¾ªç¯å‡ºé”™: {e}")
                time.sleep(interval_seconds)


# å…¨å±€å†…å­˜ä¼˜åŒ–å™¨å®ä¾‹
_global_memory_optimizer = None

def get_memory_optimizer() -> GPUMemoryOptimizer:
    """è·å–å…¨å±€å†…å­˜ä¼˜åŒ–å™¨å®ä¾‹"""
    global _global_memory_optimizer
    if _global_memory_optimizer is None:
        _global_memory_optimizer = GPUMemoryOptimizer()
    return _global_memory_optimizer


def cleanup_gpu_memory(device_id: str, aggressive: bool = False) -> bool:
    """ä¾¿æ·çš„GPUå†…å­˜æ¸…ç†å‡½æ•°"""
    optimizer = get_memory_optimizer()
    level = MemoryCleanupLevel.AGGRESSIVE if aggressive else MemoryCleanupLevel.STANDARD
    return optimizer.cleanup_device_memory(device_id, level)


def optimize_training_memory(device_id: str, model_size_mb: float, batch_size: int) -> Dict[str, Any]:
    """ä¾¿æ·çš„è®­ç»ƒå†…å­˜ä¼˜åŒ–å‡½æ•°"""
    optimizer = get_memory_optimizer()
    return optimizer.optimize_memory_for_training(device_id, model_size_mb, batch_size)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("æµ‹è¯•GPUå†…å­˜ä¼˜åŒ–å™¨...")
    
    optimizer = get_memory_optimizer()
    
    # æµ‹è¯•CPUå†…å­˜æŠ¥å‘Š
    cpu_report = optimizer.get_memory_usage_report("cpu")
    print(f"CPUå†…å­˜æŠ¥å‘Š: ä½¿ç”¨ç‡ {cpu_report.usage_percent:.1f}%")
    
    # æµ‹è¯•GPUå†…å­˜æŠ¥å‘Š
    if torch.cuda.is_available():
        gpu_report = optimizer.get_memory_usage_report("cuda:0")
        print(f"GPUå†…å­˜æŠ¥å‘Š: ä½¿ç”¨ç‡ {gpu_report.usage_percent:.1f}%")
        print(f"å»ºè®®: {gpu_report.recommendations}")
        
        # æµ‹è¯•å†…å­˜æ¸…ç†
        success = optimizer.cleanup_device_memory("cuda:0", MemoryCleanupLevel.STANDARD)
        print(f"å†…å­˜æ¸…ç†ç»“æœ: {success}")
    
    print("æµ‹è¯•å®Œæˆ")