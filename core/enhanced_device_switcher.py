# -*- coding: utf-8 -*-
"""
å¢å¼ºå‹GPUåˆ‡æ¢ç®¡ç†å™¨
ç¡®ä¿å¯è§†åŒ–ç•Œé¢åˆ‡æ¢GPUæ—¶ï¼Œç³»ç»Ÿç«‹å³å“åº”å¹¶æ­£ç¡®ä½¿ç”¨æ–°é€‰æ‹©çš„GPUè¿›è¡Œå¤„ç†

ä¸»è¦åŠŸèƒ½ï¼š
1. å¼ºåˆ¶æ¸…ç†æ—§GPUå†…å­˜å’ŒçŠ¶æ€
2. ç«‹å³æ¿€æ´»æ–°GPUè®¾å¤‡
3. åŒæ­¥æ‰€æœ‰ç»„ä»¶çš„è®¾å¤‡çŠ¶æ€  
4. ç›‘æ§è®¾å¤‡åˆ‡æ¢è¿‡ç¨‹
5. æä¾›æ•…éšœæ¢å¤æœºåˆ¶
"""

import os
import sys
import time
import threading
import gc
import logging
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass
from enum import Enum

import torch
import torch.nn as nn

try:
    import pynvml
    PYNVML_AVAILABLE = True
except ImportError:
    PYNVML_AVAILABLE = False

from PyQt5.QtCore import QObject, pyqtSignal, QTimer

# å¯¼å…¥GPUå†…å­˜ä¼˜åŒ–å™¨
try:
    from core.gpu_memory_optimizer import get_memory_optimizer, MemoryCleanupLevel
    MEMORY_OPTIMIZER_AVAILABLE = True
except ImportError:
    MEMORY_OPTIMIZER_AVAILABLE = False


class DeviceSwitchStatus(Enum):
    """è®¾å¤‡åˆ‡æ¢çŠ¶æ€æšä¸¾"""
    IDLE = "idle"
    SWITCHING = "switching"
    SUCCESS = "success"
    FAILED = "failed"
    ROLLBACK = "rollback"


@dataclass
class DeviceSwitchContext:
    """è®¾å¤‡åˆ‡æ¢ä¸Šä¸‹æ–‡ä¿¡æ¯"""
    old_device: str
    new_device: str
    timestamp: float
    models_to_move: List[Any]
    memory_cleared: bool = False
    switch_completed: bool = False
    error_message: Optional[str] = None


class EnhancedDeviceSwitcher(QObject):
    """
    å¢å¼ºå‹è®¾å¤‡åˆ‡æ¢ç®¡ç†å™¨
    
    ç‰¹æ€§ï¼š
    - å¼ºåˆ¶å†…å­˜æ¸…ç†å’Œè®¾å¤‡çŠ¶æ€é‡ç½®
    - å®æ—¶ç›‘æ§è®¾å¤‡åˆ‡æ¢è¿›åº¦
    - è‡ªåŠ¨æ•…éšœæ¢å¤å’Œå›æ»š
    - åŒæ­¥æ‰€æœ‰ç›¸å…³ç»„ä»¶
    - æä¾›è¯¦ç»†çš„åˆ‡æ¢æ—¥å¿—
    """
    
    # ä¿¡å·å®šä¹‰
    device_switch_started = pyqtSignal(str, str)  # old_device, new_device
    device_switch_progress = pyqtSignal(int)      # progress percentage
    device_switch_completed = pyqtSignal(str)     # new_device
    device_switch_failed = pyqtSignal(str, str)   # error_message, fallback_device
    memory_cleared = pyqtSignal(str)              # device_id
    
    def __init__(self, parent=None):
        super().__init__(parent)
        self.current_device = "cpu"
        self.switch_status = DeviceSwitchStatus.IDLE
        self.switch_context: Optional[DeviceSwitchContext] = None
        self.registered_models: List[Any] = []
        self.registered_components: Dict[str, Any] = {}
        self.switch_callbacks: List[Callable] = []
        self.logger = self._setup_logger()
        
        # ç›‘æ§å®šæ—¶å™¨
        self.monitor_timer = QTimer(self)
        self.monitor_timer.timeout.connect(self._monitor_device_health)
        self.monitor_timer.start(5000)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡è®¾å¤‡å¥åº·çŠ¶æ€
        
        # åˆå§‹åŒ–GPUç›‘æ§
        if PYNVML_AVAILABLE:
            try:
                pynvml.nvmlInit()
            except Exception as e:
                self.logger.warning(f"æ— æ³•åˆå§‹åŒ–NVML: {e}")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®ä¸“ç”¨æ—¥å¿—å™¨"""
        logger = logging.getLogger("EnhancedDeviceSwitcher")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [è®¾å¤‡åˆ‡æ¢å™¨] %(levelname)s: %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def register_model(self, model: Any) -> None:
        """æ³¨å†Œéœ€è¦è·Ÿéšè®¾å¤‡åˆ‡æ¢çš„æ¨¡å‹"""
        if model not in self.registered_models:
            self.registered_models.append(model)
            self.logger.info(f"æ¨¡å‹å·²æ³¨å†Œåˆ°è®¾å¤‡åˆ‡æ¢å™¨: {type(model).__name__}")
    
    def unregister_model(self, model: Any) -> None:
        """å–æ¶ˆæ³¨å†Œæ¨¡å‹"""
        if model in self.registered_models:
            self.registered_models.remove(model)
            self.logger.info(f"æ¨¡å‹å·²ä»è®¾å¤‡åˆ‡æ¢å™¨å–æ¶ˆæ³¨å†Œ: {type(model).__name__}")
    
    def register_component(self, name: str, component: Any) -> None:
        """æ³¨å†Œéœ€è¦é€šçŸ¥è®¾å¤‡åˆ‡æ¢çš„ç»„ä»¶"""
        self.registered_components[name] = component
        self.logger.info(f"ç»„ä»¶å·²æ³¨å†Œ: {name}")
    
    def add_switch_callback(self, callback: Callable[[str, str], None]) -> None:
        """æ·»åŠ è®¾å¤‡åˆ‡æ¢å›è°ƒå‡½æ•°"""
        if callback not in self.switch_callbacks:
            self.switch_callbacks.append(callback)
    
    def switch_device(self, new_device: str, force: bool = False) -> bool:
        """
        æ‰§è¡Œè®¾å¤‡åˆ‡æ¢
        
        Args:
            new_device: ç›®æ ‡è®¾å¤‡ID (å¦‚ 'cuda:0', 'cuda:1', 'cpu')
            force: æ˜¯å¦å¼ºåˆ¶åˆ‡æ¢ï¼ˆå³ä½¿å½“å‰æ­£åœ¨åˆ‡æ¢ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸå¯åŠ¨åˆ‡æ¢è¿‡ç¨‹
        """
        if not force and self.switch_status == DeviceSwitchStatus.SWITCHING:
            self.logger.warning("è®¾å¤‡åˆ‡æ¢æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆ")
            return False
        
        if new_device == self.current_device:
            self.logger.info(f"è®¾å¤‡æœªå‘ç”Ÿå˜åŒ–: {new_device}")
            return True
        
        # éªŒè¯æ–°è®¾å¤‡çš„æœ‰æ•ˆæ€§
        if not self._validate_device(new_device):
            self.logger.error(f"æ— æ•ˆçš„è®¾å¤‡ID: {new_device}")
            return False
        
        # åˆ›å»ºåˆ‡æ¢ä¸Šä¸‹æ–‡
        self.switch_context = DeviceSwitchContext(
            old_device=self.current_device,
            new_device=new_device,
            timestamp=time.time(),
            models_to_move=self.registered_models.copy()
        )
        
        self.switch_status = DeviceSwitchStatus.SWITCHING
        self.device_switch_started.emit(self.current_device, new_device)
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œåˆ‡æ¢
        switch_thread = threading.Thread(
            target=self._perform_device_switch,
            daemon=True
        )
        switch_thread.start()
        
        return True
    
    def _validate_device(self, device_id: str) -> bool:
        """éªŒè¯è®¾å¤‡IDçš„æœ‰æ•ˆæ€§"""
        if device_id == "cpu":
            return True
        
        if device_id.startswith("cuda:"):
            try:
                gpu_index = int(device_id.split(":")[1])
                return 0 <= gpu_index < torch.cuda.device_count()
            except (ValueError, IndexError):
                return False
        
        return False
    
    def _perform_device_switch(self) -> None:
        """åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå®é™…çš„è®¾å¤‡åˆ‡æ¢"""
        context = self.switch_context
        if not context:
            return
        
        try:
            self.logger.info(f"ğŸ”„ å¼€å§‹è®¾å¤‡åˆ‡æ¢: {context.old_device} â†’ {context.new_device}")
            
            # é˜¶æ®µ1: æ¸…ç†æ—§è®¾å¤‡å†…å­˜ (0-30%)
            self.device_switch_progress.emit(10)
            self._cleanup_old_device(context.old_device)
            context.memory_cleared = True
            self.memory_cleared.emit(context.old_device)
            
            # é˜¶æ®µ2: æ¿€æ´»æ–°è®¾å¤‡ (30-60%)
            self.device_switch_progress.emit(40)
            self._activate_new_device(context.new_device)
            
            # é˜¶æ®µ3: è¿ç§»æ¨¡å‹ (60-80%)
            self.device_switch_progress.emit(60)
            self._migrate_models(context)
            
            # é˜¶æ®µ4: é€šçŸ¥ç»„ä»¶å’Œå›è°ƒ (80-95%)
            self.device_switch_progress.emit(80)
            self._notify_components(context)
            
            # é˜¶æ®µ5: éªŒè¯åˆ‡æ¢ç»“æœ (95-100%)
            self.device_switch_progress.emit(95)
            if self._verify_switch_success(context.new_device):
                self.current_device = context.new_device
                context.switch_completed = True
                self.switch_status = DeviceSwitchStatus.SUCCESS
                self.device_switch_progress.emit(100)
                self.device_switch_completed.emit(context.new_device)
                self.logger.info(f"âœ… è®¾å¤‡åˆ‡æ¢æˆåŠŸå®Œæˆ: {context.new_device}")
            else:
                raise Exception("è®¾å¤‡åˆ‡æ¢éªŒè¯å¤±è´¥")
                
        except Exception as e:
            self.logger.error(f"âŒ è®¾å¤‡åˆ‡æ¢å¤±è´¥: {str(e)}")
            context.error_message = str(e)
            self.switch_status = DeviceSwitchStatus.FAILED
            
            # å°è¯•å›æ»šåˆ°åŸè®¾å¤‡
            fallback_device = self._attempt_rollback(context)
            self.device_switch_failed.emit(str(e), fallback_device)
    
    def _cleanup_old_device(self, old_device: str) -> None:
        """å½»åº•æ¸…ç†æ—§è®¾å¤‡çš„å†…å­˜å’ŒçŠ¶æ€ - å¢å¼ºç‰ˆ"""
        self.logger.info(f"ğŸ§¹ æ¸…ç†æ—§è®¾å¤‡: {old_device}")
        
        # ä½¿ç”¨å†…å­˜ä¼˜åŒ–å™¨è¿›è¡Œæ·±åº¦æ¸…ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if MEMORY_OPTIMIZER_AVAILABLE:
            try:
                memory_optimizer = get_memory_optimizer()
                success = memory_optimizer.cleanup_device_memory(
                    old_device, 
                    MemoryCleanupLevel.AGGRESSIVE,  # ä½¿ç”¨æ¿€è¿›æ¸…ç†çº§åˆ«
                    force=True
                )
                if success:
                    self.logger.info(f"âœ… ä½¿ç”¨å†…å­˜ä¼˜åŒ–å™¨æˆåŠŸæ¸…ç†è®¾å¤‡: {old_device}")
                    return
                else:
                    self.logger.warning(f"å†…å­˜ä¼˜åŒ–å™¨æ¸…ç†å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ¸…ç†")
            except Exception as e:
                self.logger.warning(f"å†…å­˜ä¼˜åŒ–å™¨æ¸…ç†å‡ºé”™: {e}ï¼Œå›é€€åˆ°åŸºç¡€æ¸…ç†")
        
        # åŸºç¡€æ¸…ç†æ–¹æ³•ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ä½œä¸ºå¤‡ç”¨ï¼‰
        if old_device.startswith("cuda:") and torch.cuda.is_available():
            try:
                gpu_index = int(old_device.split(":")[1])
                
                # åˆ‡æ¢åˆ°ç›®æ ‡GPUè¿›è¡Œæ¸…ç†
                original_device = torch.cuda.current_device()
                torch.cuda.set_device(gpu_index)
                
                # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰ç¼“å­˜
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                
                # é‡ç½®å†…å­˜ç»Ÿè®¡
                torch.cuda.reset_peak_memory_stats(gpu_index)
                torch.cuda.reset_accumulated_memory_stats(gpu_index)
                
                # æ¢å¤åŸè®¾å¤‡
                if original_device != gpu_index:
                    torch.cuda.set_device(original_device)
                
                self.logger.info(f"GPU {gpu_index} åŸºç¡€å†…å­˜æ¸…ç†å®Œæˆ")
                
            except Exception as e:
                self.logger.warning(f"æ¸…ç†GPUå†…å­˜æ—¶å‡ºé”™: {e}")
        
        # æ‰§è¡ŒPythonåƒåœ¾å›æ”¶
        for _ in range(3):  # å¤šæ¬¡åƒåœ¾å›æ”¶ç¡®ä¿å½»åº•æ¸…ç†
            gc.collect()
        
        time.sleep(0.1)  # ç»™ç³»ç»Ÿä¸€ç‚¹æ—¶é—´å®Œæˆæ¸…ç†
    
    def _activate_new_device(self, new_device: str) -> None:
        """æ¿€æ´»æ–°è®¾å¤‡"""
        self.logger.info(f"âš¡ æ¿€æ´»æ–°è®¾å¤‡: {new_device}")
        
        if new_device.startswith("cuda:") and torch.cuda.is_available():
            try:
                gpu_index = int(new_device.split(":")[1])
                
                # è®¾ç½®æ–°çš„å½“å‰è®¾å¤‡
                torch.cuda.set_device(gpu_index)
                
                # é¢„çƒ­æ–°è®¾å¤‡ï¼ˆåˆ›å»ºä¸€ä¸ªå°å¼ é‡æ¥åˆå§‹åŒ–CUDAä¸Šä¸‹æ–‡ï¼‰
                with torch.cuda.device(gpu_index):
                    warmup_tensor = torch.randn(10, 10, device=new_device)
                    _ = warmup_tensor @ warmup_tensor.T  # ç®€å•è®¡ç®—æ¥é¢„çƒ­
                    del warmup_tensor
                    torch.cuda.synchronize()
                
                self.logger.info(f"GPU {gpu_index} å·²æ¿€æ´»å¹¶é¢„çƒ­")
                
            except Exception as e:
                raise Exception(f"æ¿€æ´»GPUè®¾å¤‡å¤±è´¥: {e}")
    
    def _migrate_models(self, context: DeviceSwitchContext) -> None:
        """è¿ç§»æ³¨å†Œçš„æ¨¡å‹åˆ°æ–°è®¾å¤‡"""
        if not context.models_to_move:
            return
        
        self.logger.info(f"ğŸ“¦ è¿ç§» {len(context.models_to_move)} ä¸ªæ¨¡å‹åˆ°æ–°è®¾å¤‡")
        
        for i, model in enumerate(context.models_to_move):
            try:
                if hasattr(model, 'to'):
                    # å¯¹äºåŒ…è£…åœ¨DataParallelä¸­çš„æ¨¡å‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                    if isinstance(model, (nn.DataParallel, nn.parallel.DistributedDataParallel)):
                        # é‡æ–°åˆ›å»ºDataParallelåŒ…è£…
                        underlying_model = model.module
                        underlying_model = underlying_model.to(context.new_device)
                        
                        # å¦‚æœæ–°è®¾å¤‡æ˜¯GPUä¸”å¯ç”¨äº†å¤šGPUï¼Œé‡æ–°åŒ…è£…
                        if context.new_device.startswith("cuda:"):
                            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦é‡æ–°è®¾ç½®å¤šGPU
                            pass
                    else:
                        model = model.to(context.new_device)
                    
                    self.logger.debug(f"æ¨¡å‹ {type(model).__name__} å·²è¿ç§»åˆ° {context.new_device}")
                else:
                    self.logger.warning(f"æ¨¡å‹ {type(model).__name__} ä¸æ”¯æŒè®¾å¤‡è¿ç§»")
                    
            except Exception as e:
                self.logger.error(f"è¿ç§»æ¨¡å‹ {type(model).__name__} å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†å…¶ä»–æ¨¡å‹ï¼Œä¸å› å•ä¸ªæ¨¡å‹å¤±è´¥è€Œä¸­æ–­
        
        # å¼ºåˆ¶åŒæ­¥
        if context.new_device.startswith("cuda:"):
            torch.cuda.synchronize()
    
    def _notify_components(self, context: DeviceSwitchContext) -> None:
        """é€šçŸ¥æ‰€æœ‰æ³¨å†Œçš„ç»„ä»¶è®¾å¤‡å·²åˆ‡æ¢"""
        self.logger.info(f"ğŸ“¢ é€šçŸ¥ {len(self.registered_components)} ä¸ªç»„ä»¶")
        
        for name, component in self.registered_components.items():
            try:
                if hasattr(component, 'on_device_changed'):
                    component.on_device_changed(context.old_device, context.new_device)
                elif hasattr(component, 'set_device'):
                    component.set_device(context.new_device)
                
                self.logger.debug(f"ç»„ä»¶ {name} å·²æ”¶åˆ°è®¾å¤‡åˆ‡æ¢é€šçŸ¥")
            except Exception as e:
                self.logger.error(f"é€šçŸ¥ç»„ä»¶ {name} æ—¶å‡ºé”™: {e}")
        
        # æ‰§è¡Œå›è°ƒå‡½æ•°
        for callback in self.switch_callbacks:
            try:
                callback(context.old_device, context.new_device)
            except Exception as e:
                self.logger.error(f"æ‰§è¡Œåˆ‡æ¢å›è°ƒæ—¶å‡ºé”™: {e}")
    
    def _verify_switch_success(self, new_device: str) -> bool:
        """éªŒè¯è®¾å¤‡åˆ‡æ¢æ˜¯å¦æˆåŠŸ"""
        try:
            if new_device == "cpu":
                # CPUè®¾å¤‡éªŒè¯
                test_tensor = torch.randn(10, 10)
                return test_tensor.device.type == "cpu"
            
            elif new_device.startswith("cuda:"):
                gpu_index = int(new_device.split(":")[1])
                
                # éªŒè¯CUDAè®¾å¤‡
                if not torch.cuda.is_available():
                    return False
                
                # éªŒè¯è®¾å¤‡ç´¢å¼•
                if gpu_index >= torch.cuda.device_count():
                    return False
                
                # åˆ›å»ºæµ‹è¯•å¼ é‡å¹¶éªŒè¯
                test_tensor = torch.randn(10, 10, device=new_device)
                result = test_tensor @ test_tensor.T
                
                # éªŒè¯è®¡ç®—ç»“æœåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                is_correct_device = (
                    test_tensor.device.type == "cuda" and
                    test_tensor.device.index == gpu_index and
                    result.device == test_tensor.device
                )
                
                del test_tensor, result
                return is_correct_device
            
            return False
            
        except Exception as e:
            self.logger.error(f"éªŒè¯è®¾å¤‡åˆ‡æ¢æ—¶å‡ºé”™: {e}")
            return False
    
    def _attempt_rollback(self, context: DeviceSwitchContext) -> str:
        """å°è¯•å›æ»šåˆ°å®‰å…¨çš„è®¾å¤‡"""
        self.switch_status = DeviceSwitchStatus.ROLLBACK
        self.logger.warning(f"ğŸ”„ å°è¯•å›æ»šåˆ°å®‰å…¨è®¾å¤‡")
        
        # é¦–å…ˆå°è¯•å›æ»šåˆ°åŸè®¾å¤‡
        if self._validate_device(context.old_device):
            try:
                self._activate_new_device(context.old_device)
                self.current_device = context.old_device
                self.logger.info(f"æˆåŠŸå›æ»šåˆ°åŸè®¾å¤‡: {context.old_device}")
                return context.old_device
            except Exception as e:
                self.logger.error(f"å›æ»šåˆ°åŸè®¾å¤‡å¤±è´¥: {e}")
        
        # å¦‚æœåŸè®¾å¤‡ä¸å¯ç”¨ï¼Œå›æ»šåˆ°CPU
        try:
            self.current_device = "cpu"
            self.logger.info("å›æ»šåˆ°CPUè®¾å¤‡")
            return "cpu"
        except Exception as e:
            self.logger.critical(f"è¿CPUè®¾å¤‡éƒ½æ— æ³•ä½¿ç”¨: {e}")
            return "cpu"  # ä»ç„¶è¿”å›CPUä½œä¸ºæœ€åçš„é€‰æ‹©
    
    def _monitor_device_health(self) -> None:
        """ç›‘æ§å½“å‰è®¾å¤‡çš„å¥åº·çŠ¶æ€"""
        if self.switch_status == DeviceSwitchStatus.SWITCHING:
            return  # åˆ‡æ¢è¿‡ç¨‹ä¸­è·³è¿‡ç›‘æ§
        
        try:
            if self.current_device.startswith("cuda:") and torch.cuda.is_available():
                gpu_index = int(self.current_device.split(":")[1])
                
                # æ£€æŸ¥GPUæ˜¯å¦ä»ç„¶å¯ç”¨
                if gpu_index >= torch.cuda.device_count():
                    self.logger.warning(f"å½“å‰GPU {gpu_index} ä¸å†å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
                    self.switch_device("cpu", force=True)
                    return
                
                # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
                if PYNVML_AVAILABLE:
                    try:
                        handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)
                        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                        usage_percent = (mem_info.used / mem_info.total) * 100
                        
                        if usage_percent > 95:
                            self.logger.warning(f"GPU {gpu_index} å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {usage_percent:.1f}%")
                    except Exception:
                        pass  # å¿½ç•¥NVMLé”™è¯¯
                        
        except Exception as e:
            self.logger.debug(f"è®¾å¤‡å¥åº·ç›‘æ§å‡ºé”™: {e}")
    
    def get_current_device(self) -> str:
        """è·å–å½“å‰è®¾å¤‡ID"""
        return self.current_device
    
    def get_switch_status(self) -> DeviceSwitchStatus:
        """è·å–åˆ‡æ¢çŠ¶æ€"""
        return self.switch_status
    
    def force_cleanup_all_devices(self) -> None:
        """å¼ºåˆ¶æ¸…ç†æ‰€æœ‰GPUè®¾å¤‡çš„å†…å­˜ - å¢å¼ºç‰ˆ"""
        self.logger.info("ğŸ§¹ å¼ºåˆ¶æ¸…ç†æ‰€æœ‰GPUè®¾å¤‡å†…å­˜")
        
        # ä½¿ç”¨å†…å­˜ä¼˜åŒ–å™¨è¿›è¡Œæ·±åº¦æ¸…ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if MEMORY_OPTIMIZER_AVAILABLE:
            try:
                memory_optimizer = get_memory_optimizer()
                
                # æ¸…ç†æ‰€æœ‰GPUè®¾å¤‡
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        device_id = f"cuda:{i}"
                        success = memory_optimizer.cleanup_device_memory(
                            device_id, 
                            MemoryCleanupLevel.DEEP,  # ä½¿ç”¨æœ€æ·±åº¦æ¸…ç†
                            force=True
                        )
                        if success:
                            self.logger.info(f"âœ… è®¾å¤‡ {device_id} æ·±åº¦æ¸…ç†å®Œæˆ")
                        else:
                            self.logger.warning(f"âš ï¸ è®¾å¤‡ {device_id} æ¸…ç†å¤±è´¥")
                
                # æ¸…ç†CPUå†…å­˜
                memory_optimizer.cleanup_device_memory("cpu", MemoryCleanupLevel.AGGRESSIVE, force=True)
                self.logger.info("âœ… æ‰€æœ‰è®¾å¤‡å†…å­˜ä¼˜åŒ–æ¸…ç†å®Œæˆ")
                return
                
            except Exception as e:
                self.logger.warning(f"å†…å­˜ä¼˜åŒ–å™¨æ¸…ç†å‡ºé”™: {e}ï¼Œå›é€€åˆ°åŸºç¡€æ¸…ç†")
        
        # åŸºç¡€æ¸…ç†æ–¹æ³•ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ä½œä¸ºå¤‡ç”¨ï¼‰
        if not torch.cuda.is_available():
            self.logger.info("CUDAä¸å¯ç”¨ï¼Œä»…æ‰§è¡ŒCPUå†…å­˜æ¸…ç†")
            for _ in range(3):
                gc.collect()
            return
        
        original_device = torch.cuda.current_device()
        
        for i in range(torch.cuda.device_count()):
            try:
                torch.cuda.set_device(i)
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
                torch.cuda.reset_peak_memory_stats(i)
                torch.cuda.reset_accumulated_memory_stats(i)
                self.logger.info(f"GPU {i} åŸºç¡€å†…å­˜æ¸…ç†å®Œæˆ")
            except Exception as e:
                self.logger.warning(f"æ¸…ç†GPU {i} æ—¶å‡ºé”™: {e}")
        
        # æ¢å¤åŸè®¾å¤‡
        try:
            torch.cuda.set_device(original_device)
        except Exception:
            pass
        
        # æ‰§è¡Œåƒåœ¾å›æ”¶
        for _ in range(3):
            gc.collect()
        
        self.logger.info("âœ… æ‰€æœ‰è®¾å¤‡åŸºç¡€æ¸…ç†å®Œæˆ")
    
    def get_device_memory_info(self, device_id: str = None) -> Dict[str, Any]:
        """è·å–è®¾å¤‡å†…å­˜ä¿¡æ¯"""
        if device_id is None:
            device_id = self.current_device
        
        info = {"device": device_id, "available": False}
        
        try:
            if device_id == "cpu":
                import psutil
                memory = psutil.virtual_memory()
                info.update({
                    "available": True,
                    "type": "cpu",
                    "total_mb": memory.total / (1024**2),
                    "used_mb": memory.used / (1024**2),
                    "free_mb": memory.available / (1024**2),
                    "usage_percent": memory.percent
                })
            
            elif device_id.startswith("cuda:") and torch.cuda.is_available():
                gpu_index = int(device_id.split(":")[1])
                if gpu_index < torch.cuda.device_count():
                    allocated = torch.cuda.memory_allocated(gpu_index) / (1024**2)
                    reserved = torch.cuda.memory_reserved(gpu_index) / (1024**2)
                    
                    if PYNVML_AVAILABLE:
                        try:
                            handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)
                            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                            total = mem_info.total / (1024**2)
                            used = mem_info.used / (1024**2)
                            free = mem_info.free / (1024**2)
                        except Exception:
                            props = torch.cuda.get_device_properties(gpu_index)
                            total = props.total_memory / (1024**2)
                            used = allocated
                            free = total - used
                    else:
                        props = torch.cuda.get_device_properties(gpu_index)
                        total = props.total_memory / (1024**2)
                        used = allocated
                        free = total - used
                    
                    info.update({
                        "available": True,
                        "type": "gpu",
                        "total_mb": total,
                        "used_mb": used,
                        "free_mb": free,
                        "allocated_mb": allocated,
                        "reserved_mb": reserved,
                        "usage_percent": (used / total) * 100 if total > 0 else 0
                    })
        
        except Exception as e:
            info["error"] = str(e)
        
        return info


# å…¨å±€è®¾å¤‡åˆ‡æ¢å™¨å®ä¾‹
_global_device_switcher = None

def get_device_switcher() -> EnhancedDeviceSwitcher:
    """è·å–å…¨å±€è®¾å¤‡åˆ‡æ¢å™¨å®ä¾‹"""
    global _global_device_switcher
    if _global_device_switcher is None:
        _global_device_switcher = EnhancedDeviceSwitcher()
    return _global_device_switcher


def cleanup_global_device_switcher():
    """æ¸…ç†å…¨å±€è®¾å¤‡åˆ‡æ¢å™¨"""
    global _global_device_switcher
    if _global_device_switcher is not None:
        _global_device_switcher.force_cleanup_all_devices()
        _global_device_switcher = None


# ä¾¿æ·å‡½æ•°
def switch_to_device(device_id: str, force: bool = False) -> bool:
    """ä¾¿æ·çš„è®¾å¤‡åˆ‡æ¢å‡½æ•°"""
    switcher = get_device_switcher()
    return switcher.switch_device(device_id, force)


def get_current_processing_device() -> str:
    """è·å–å½“å‰å¤„ç†è®¾å¤‡"""
    switcher = get_device_switcher()
    return switcher.get_current_device()


def register_model_for_switching(model: Any) -> None:
    """æ³¨å†Œæ¨¡å‹ç”¨äºè‡ªåŠ¨è®¾å¤‡åˆ‡æ¢"""
    switcher = get_device_switcher()
    switcher.register_model(model)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("æµ‹è¯•å¢å¼ºå‹è®¾å¤‡åˆ‡æ¢å™¨...")
    
    switcher = get_device_switcher()
    
    # æµ‹è¯•è®¾å¤‡éªŒè¯
    print(f"CPUå¯ç”¨: {switcher._validate_device('cpu')}")
    if torch.cuda.is_available():
        print(f"CUDA:0å¯ç”¨: {switcher._validate_device('cuda:0')}")
    
    # æµ‹è¯•å†…å­˜ä¿¡æ¯
    cpu_info = switcher.get_device_memory_info("cpu")
    print(f"CPUå†…å­˜ä¿¡æ¯: {cpu_info}")
    
    if torch.cuda.is_available():
        gpu_info = switcher.get_device_memory_info("cuda:0")
        print(f"GPUå†…å­˜ä¿¡æ¯: {gpu_info}")
    
    print("æµ‹è¯•å®Œæˆ")