# -*- coding: utf-8 -*-
"""
è®¡ç®—æ•ˆç‡æœ€å¤§åŒ–ä¼˜åŒ–å™¨
æè‡´ä¼˜åŒ–GPUè®¡ç®—æ€§èƒ½ï¼Œç¡®ä¿ç³»ç»Ÿå‘æŒ¥ç¡¬ä»¶çš„æœ€å¤§æ½œåŠ›

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–
2. å¹¶è¡Œè®¡ç®—ç­–ç•¥
3. å†…å­˜å¸¦å®½æœ€å¤§åŒ–
4. ç®—æ³•æ•ˆç‡ä¼˜åŒ–
5. èµ„æºåŠ¨æ€è°ƒåº¦
6. æ€§èƒ½å®æ—¶ç›‘æ§
"""

import os
import sys
import time
import math
import threading
import queue
import logging
import gc
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import numpy as np

try:
    import pynvml
    PYNVML_AVAILABLE = True
except ImportError:
    PYNVML_AVAILABLE = False

try:
    from torch.profiler import profile, record_function, ProfilerActivity
    PROFILER_AVAILABLE = True
except ImportError:
    PROFILER_AVAILABLE = False


class ComputeOptimizationLevel(Enum):
    """è®¡ç®—ä¼˜åŒ–çº§åˆ«"""
    CONSERVATIVE = "conservative"  # ä¿å®ˆä¼˜åŒ–ï¼Œç¡®ä¿ç¨³å®š
    BALANCED = "balanced"         # å¹³è¡¡ä¼˜åŒ–ï¼Œæ€§èƒ½ä¸ç¨³å®šå¹¶é‡
    AGGRESSIVE = "aggressive"     # æ¿€è¿›ä¼˜åŒ–ï¼Œæœ€å¤§åŒ–æ€§èƒ½
    EXTREME = "extreme"          # æé™ä¼˜åŒ–ï¼Œæ¦¨å–æ‰€æœ‰æ€§èƒ½


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    throughput: float = 0.0        # ååé‡ (samples/sec)
    latency: float = 0.0           # å»¶è¿Ÿ (ms)
    gpu_utilization: float = 0.0   # GPUåˆ©ç”¨ç‡ (%)
    memory_efficiency: float = 0.0  # å†…å­˜æ•ˆç‡ (%)
    compute_efficiency: float = 0.0 # è®¡ç®—æ•ˆç‡ (%)
    bandwidth_utilization: float = 0.0  # å¸¦å®½åˆ©ç”¨ç‡ (%)
    energy_efficiency: float = 0.0 # èƒ½æ•ˆ (performance/watt)
    bottlenecks: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


@dataclass
class OptimizationConfig:
    """ä¼˜åŒ–é…ç½®"""
    optimization_level: ComputeOptimizationLevel = ComputeOptimizationLevel.BALANCED
    max_batch_size: int = 128
    prefetch_factor: int = 4
    num_workers: int = 4
    pin_memory: bool = True
    mixed_precision: bool = True
    compile_model: bool = True
    gradient_checkpointing: bool = False
    tensor_parallel: bool = True
    pipeline_parallel: bool = False
    memory_pool_fraction: float = 0.9
    enable_profiling: bool = False


class DynamicBatchOptimizer:
    """
    åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–å™¨
    
    æ ¹æ®GPUå†…å­˜ã€è®¡ç®—èƒ½åŠ›å’Œæ•°æ®ç‰¹å¾åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼Œ
    æœ€å¤§åŒ–GPUåˆ©ç”¨ç‡å’Œååé‡
    """
    
    def __init__(self, device: str, model: nn.Module):
        self.device = torch.device(device)
        self.model = model
        self.logger = self._setup_logger()
        
        # æ€§èƒ½å†å²è®°å½•
        self.performance_history: List[PerformanceMetrics] = []
        self.optimal_batch_sizes: Dict[str, int] = {}
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°
        self.current_batch_size = 1
        self.max_tested_batch_size = 1
        self.performance_threshold = 0.95  # æ€§èƒ½é˜ˆå€¼
        
        # GPUå±æ€§
        self._analyze_gpu_capabilities()
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger("DynamicBatchOptimizer")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [æ‰¹å¤„ç†ä¼˜åŒ–å™¨] %(levelname)s: %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _analyze_gpu_capabilities(self):
        """åˆ†æGPUè®¡ç®—èƒ½åŠ›"""
        if self.device.type == 'cuda':
            try:
                props = torch.cuda.get_device_properties(self.device)
                self.gpu_memory_gb = props.total_memory / (1024**3)
                self.sm_count = props.multi_processor_count
                self.max_threads_per_sm = props.max_threads_per_multi_processor
                self.compute_capability = f"{props.major}.{props.minor}"
                
                # è®¡ç®—ç†è®ºå³°å€¼æ€§èƒ½
                self.theoretical_max_batch = self._estimate_max_batch_size()
                
                self.logger.info(f"GPUåˆ†æå®Œæˆ: {props.name}")
                self.logger.info(f"  æ˜¾å­˜: {self.gpu_memory_gb:.1f} GB")
                self.logger.info(f"  SMæ•°é‡: {self.sm_count}")
                self.logger.info(f"  è®¡ç®—èƒ½åŠ›: {self.compute_capability}")
                self.logger.info(f"  ç†è®ºæœ€å¤§æ‰¹æ¬¡: {self.theoretical_max_batch}")
                
            except Exception as e:
                self.logger.warning(f"GPUèƒ½åŠ›åˆ†æå¤±è´¥: {e}")
                self.gpu_memory_gb = 8.0  # é»˜è®¤å€¼
                self.sm_count = 20
                self.theoretical_max_batch = 32
    
    def _estimate_max_batch_size(self) -> int:
        """ä¼°ç®—æœ€å¤§æ‰¹æ¬¡å¤§å°"""
        try:
            # åŸºäºGPUå†…å­˜çš„ç²—ç•¥ä¼°ç®—
            model_size_gb = sum(p.numel() * 4 for p in self.model.parameters()) / (1024**3)
            
            # ä¿å®ˆä¼°è®¡ï¼šæ¨¡å‹ + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ¿€æ´»å€¼
            memory_per_sample_mb = 50  # æ¯ä¸ªæ ·æœ¬å¤§æ¦‚50MB (å¯è°ƒæ•´)
            available_memory_gb = self.gpu_memory_gb * 0.8  # 80%å¯ç”¨å†…å­˜
            
            estimated_batch = int((available_memory_gb * 1024 - model_size_gb * 3 * 1024) / memory_per_sample_mb)
            return max(1, min(estimated_batch, 256))  # é™åˆ¶åœ¨1-256ä¹‹é—´
            
        except Exception as e:
            self.logger.warning(f"æ‰¹æ¬¡å¤§å°ä¼°ç®—å¤±è´¥: {e}")
            return 32
    
    def find_optimal_batch_size(self, 
                               data_shape: Tuple, 
                               max_iterations: int = 10,
                               target_utilization: float = 0.85) -> int:
        """
        å¯»æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å°
        
        Args:
            data_shape: å•ä¸ªæ ·æœ¬çš„æ•°æ®å½¢çŠ¶
            max_iterations: æœ€å¤§æµ‹è¯•è¿­ä»£æ¬¡æ•°
            target_utilization: ç›®æ ‡GPUåˆ©ç”¨ç‡
            
        Returns:
            æœ€ä¼˜æ‰¹æ¬¡å¤§å°
        """
        self.logger.info(f"ğŸ” å¯»æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å° (ç›®æ ‡åˆ©ç”¨ç‡: {target_utilization*100:.1f}%)")
        
        # ç”Ÿæˆå½¢çŠ¶æ ‡è¯†ç¬¦
        shape_key = str(data_shape)
        if shape_key in self.optimal_batch_sizes:
            cached_batch = self.optimal_batch_sizes[shape_key]
            self.logger.info(f"ä½¿ç”¨ç¼“å­˜çš„æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {cached_batch}")
            return cached_batch
        
        best_batch_size = 1
        best_throughput = 0.0
        test_batch_sizes = []
        
        # ç”Ÿæˆæµ‹è¯•æ‰¹æ¬¡å¤§å°åºåˆ—
        start_batch = 1
        max_batch = min(self.theoretical_max_batch, 256)
        
        # äºŒåˆ†æœç´¢ + æŒ‡æ•°å¢é•¿
        batch = start_batch
        while batch <= max_batch and len(test_batch_sizes) < max_iterations:
            test_batch_sizes.append(batch)
            if batch < 8:
                batch += 1
            elif batch < 32:
                batch += 4
            else:
                batch = int(batch * 1.5)
        
        self.logger.info(f"æµ‹è¯•æ‰¹æ¬¡å¤§å°åºåˆ—: {test_batch_sizes}")
        
        for batch_size in test_batch_sizes:
            try:
                self.logger.info(f"ğŸ§ª æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size}")
                
                # é¢„çƒ­
                self._warmup_gpu(data_shape, batch_size)
                
                # æ€§èƒ½æµ‹è¯•
                metrics = self._benchmark_batch_size(data_shape, batch_size)
                
                self.logger.info(f"  ååé‡: {metrics.throughput:.1f} samples/sec")
                self.logger.info(f"  GPUåˆ©ç”¨ç‡: {metrics.gpu_utilization:.1f}%")
                self.logger.info(f"  å†…å­˜æ•ˆç‡: {metrics.memory_efficiency:.1f}%")
                
                # æ£€æŸ¥æ˜¯å¦è¶…å‡ºå†…å­˜é™åˆ¶
                if metrics.gpu_utilization < 0:  # è¡¨ç¤ºå†…å­˜æº¢å‡º
                    self.logger.warning(f"æ‰¹æ¬¡å¤§å° {batch_size} è¶…å‡ºå†…å­˜é™åˆ¶")
                    break
                
                # æ›´æ–°æœ€ä½³æ‰¹æ¬¡å¤§å°
                if metrics.throughput > best_throughput:
                    best_throughput = metrics.throughput
                    best_batch_size = batch_size
                
                # å¦‚æœGPUåˆ©ç”¨ç‡å·²ç»å¾ˆé«˜ä¸”æ€§èƒ½æå‡å¾®å°ï¼Œå¯ä»¥åœæ­¢
                if (metrics.gpu_utilization > target_utilization * 100 and 
                    metrics.throughput > best_throughput * 0.95):
                    self.logger.info(f"è¾¾åˆ°ç›®æ ‡åˆ©ç”¨ç‡ï¼Œåœæ­¢æœç´¢")
                    break
                    
            except torch.cuda.OutOfMemoryError:
                self.logger.warning(f"æ‰¹æ¬¡å¤§å° {batch_size} å¯¼è‡´å†…å­˜æº¢å‡º")
                break
            except Exception as e:
                self.logger.error(f"æµ‹è¯•æ‰¹æ¬¡å¤§å° {batch_size} å¤±è´¥: {e}")
                continue
            finally:
                # æ¸…ç†å†…å­˜
                torch.cuda.empty_cache()
                gc.collect()
        
        # ç¼“å­˜ç»“æœ
        self.optimal_batch_sizes[shape_key] = best_batch_size
        self.current_batch_size = best_batch_size
        
        self.logger.info(f"âœ… æ‰¾åˆ°æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {best_batch_size} (ååé‡: {best_throughput:.1f} samples/sec)")
        return best_batch_size
    
    def _warmup_gpu(self, data_shape: Tuple, batch_size: int, warmup_steps: int = 3):
        """GPUé¢„çƒ­"""
        try:
            with torch.no_grad():
                for _ in range(warmup_steps):
                    dummy_input = torch.randn(batch_size, *data_shape, device=self.device)
                    _ = self.model(dummy_input)
                    del dummy_input
                
                torch.cuda.synchronize(self.device)
        except Exception as e:
            self.logger.debug(f"é¢„çƒ­å¤±è´¥: {e}")
    
    def _benchmark_batch_size(self, data_shape: Tuple, batch_size: int, 
                            test_steps: int = 10) -> PerformanceMetrics:
        """åŸºå‡†æµ‹è¯•ç‰¹å®šæ‰¹æ¬¡å¤§å°"""
        metrics = PerformanceMetrics()
        
        try:
            # è®°å½•åˆå§‹å†…å­˜
            torch.cuda.reset_peak_memory_stats(self.device)
            initial_memory = torch.cuda.memory_allocated(self.device)
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = torch.randn(batch_size, *data_shape, device=self.device)
            
            # æ€§èƒ½æµ‹è¯•
            self.model.eval()
            torch.cuda.synchronize(self.device)
            
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(test_steps):
                    outputs = self.model(test_data)
                    del outputs
            
            torch.cuda.synchronize(self.device)
            end_time = time.time()
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            total_samples = batch_size * test_steps
            total_time = end_time - start_time
            
            metrics.throughput = total_samples / total_time
            metrics.latency = (total_time / test_steps) * 1000  # ms
            
            # å†…å­˜ä½¿ç”¨åˆ†æ
            peak_memory = torch.cuda.max_memory_allocated(self.device)
            memory_used = peak_memory - initial_memory
            
            if self.device.type == 'cuda':
                total_memory = torch.cuda.get_device_properties(self.device).total_memory
                metrics.memory_efficiency = (memory_used / total_memory) * 100
            
            # GPUåˆ©ç”¨ç‡ (ç®€åŒ–ä¼°ç®—)
            if PYNVML_AVAILABLE:
                try:
                    handle = pynvml.nvmlDeviceGetHandleByIndex(self.device.index)
                    util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                    metrics.gpu_utilization = util.gpu
                except:
                    metrics.gpu_utilization = min(95.0, metrics.memory_efficiency * 1.2)
            else:
                metrics.gpu_utilization = min(95.0, metrics.memory_efficiency * 1.2)
            
            # æ¸…ç†æµ‹è¯•æ•°æ®
            del test_data
            
        except torch.cuda.OutOfMemoryError:
            metrics.gpu_utilization = -1  # æ ‡è®°å†…å­˜æº¢å‡º
            metrics.throughput = 0
        except Exception as e:
            self.logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            metrics.throughput = 0
        
        return metrics


class ParallelComputeEngine:
    """
    å¹¶è¡Œè®¡ç®—å¼•æ“
    
    æœ€å¤§åŒ–åˆ©ç”¨å¤šGPUã€å¤šæ ¸CPUå’Œå…¶ä»–è®¡ç®—èµ„æºï¼Œ
    å®ç°æ•°æ®å¹¶è¡Œã€æ¨¡å‹å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œ
    """
    
    def __init__(self, config: OptimizationConfig):
        self.config = config
        self.logger = self._setup_logger()
        
        # æ£€æµ‹å¯ç”¨èµ„æº
        self.available_gpus = self._detect_gpus()
        self.cpu_cores = mp.cpu_count()
        
        # å¹¶è¡Œç­–ç•¥
        self.data_parallel_enabled = False
        self.model_parallel_enabled = False
        self.pipeline_parallel_enabled = False
        
        # å·¥ä½œæ± 
        self.thread_pool = ThreadPoolExecutor(max_workers=self.config.num_workers)
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger("ParallelComputeEngine")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [å¹¶è¡Œè®¡ç®—å¼•æ“] %(levelname)s: %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _detect_gpus(self) -> List[int]:
        """æ£€æµ‹å¯ç”¨GPU"""
        gpus = []
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                try:
                    # æµ‹è¯•GPUæ˜¯å¦å¯ç”¨
                    with torch.cuda.device(i):
                        test_tensor = torch.randn(10, 10, device=f'cuda:{i}')
                        del test_tensor
                    gpus.append(i)
                except Exception as e:
                    self.logger.warning(f"GPU {i} ä¸å¯ç”¨: {e}")
        
        self.logger.info(f"æ£€æµ‹åˆ° {len(gpus)} ä¸ªå¯ç”¨GPU: {gpus}")
        return gpus
    
    def setup_data_parallel(self, model: nn.Module) -> nn.Module:
        """è®¾ç½®æ•°æ®å¹¶è¡Œ"""
        if len(self.available_gpus) <= 1:
            self.logger.info("åªæœ‰ä¸€ä¸ªGPUï¼Œè·³è¿‡æ•°æ®å¹¶è¡Œ")
            return model
        
        if self.config.tensor_parallel:
            try:
                # ä½¿ç”¨DataParallelè¿›è¡Œæ•°æ®å¹¶è¡Œ
                model = nn.DataParallel(model, device_ids=self.available_gpus)
                self.data_parallel_enabled = True
                self.logger.info(f"âœ… æ•°æ®å¹¶è¡Œå·²å¯ç”¨ï¼Œä½¿ç”¨GPU: {self.available_gpus}")
                return model
            except Exception as e:
                self.logger.error(f"æ•°æ®å¹¶è¡Œè®¾ç½®å¤±è´¥: {e}")
                return model
        
        return model
    
    def optimize_dataloader(self, dataset: Dataset, batch_size: int) -> DataLoader:
        """ä¼˜åŒ–æ•°æ®åŠ è½½å™¨"""
        # åŠ¨æ€è°ƒæ•´workeræ•°é‡
        optimal_workers = min(
            self.config.num_workers,
            self.cpu_cores // 2,
            len(dataset) // batch_size + 1
        )
        
        # æ ¹æ®æ•°æ®å¤§å°è°ƒæ•´prefetch
        prefetch_factor = self.config.prefetch_factor
        if len(dataset) < 1000:
            prefetch_factor = 2
        elif len(dataset) > 10000:
            prefetch_factor = 8
        
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=optimal_workers,
            pin_memory=self.config.pin_memory and torch.cuda.is_available(),
            prefetch_factor=prefetch_factor,
            persistent_workers=optimal_workers > 0,
            drop_last=False
        )
        
        self.logger.info(f"æ•°æ®åŠ è½½å™¨ä¼˜åŒ–: workers={optimal_workers}, prefetch={prefetch_factor}")
        return dataloader
    
    def parallel_inference(self, 
                          model: nn.Module, 
                          data_batches: List[torch.Tensor],
                          device: str = None) -> List[torch.Tensor]:
        """
        å¹¶è¡Œæ¨ç†
        
        å°†æ•°æ®æ‰¹æ¬¡åˆ†å‘åˆ°å¤šä¸ªGPUè¿›è¡Œå¹¶è¡Œæ¨ç†
        """
        if len(self.available_gpus) <= 1 or len(data_batches) == 1:
            # å•GPUæˆ–å•æ‰¹æ¬¡ï¼Œç›´æ¥æ¨ç†
            device = device or f'cuda:{self.available_gpus[0]}' if self.available_gpus else 'cpu'
            results = []
            model = model.to(device)
            model.eval()
            
            with torch.no_grad():
                for batch in data_batches:
                    batch = batch.to(device)
                    output = model(batch)
                    results.append(output.cpu())
            
            return results
        
        # å¤šGPUå¹¶è¡Œæ¨ç†
        results = [None] * len(data_batches)
        futures = {}
        
        # å°†æ‰¹æ¬¡åˆ†å‘åˆ°ä¸åŒGPU
        for i, batch in enumerate(data_batches):
            gpu_id = self.available_gpus[i % len(self.available_gpus)]
            future = self.thread_pool.submit(
                self._single_gpu_inference, 
                model, batch, f'cuda:{gpu_id}', i
            )
            futures[future] = i
        
        # æ”¶é›†ç»“æœ
        for future in as_completed(futures):
            batch_idx = futures[future]
            try:
                results[batch_idx] = future.result()
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡ {batch_idx} æ¨ç†å¤±è´¥: {e}")
                results[batch_idx] = None
        
        return results
    
    def _single_gpu_inference(self, 
                             model: nn.Module, 
                             batch: torch.Tensor, 
                             device: str, 
                             batch_idx: int) -> torch.Tensor:
        """å•GPUæ¨ç†"""
        try:
            # åˆ›å»ºæ¨¡å‹å‰¯æœ¬
            model_copy = model.to(device)
            model_copy.eval()
            
            batch = batch.to(device)
            
            with torch.no_grad():
                output = model_copy(batch)
            
            return output.cpu()
            
        except Exception as e:
            self.logger.error(f"GPU {device} æ¨ç†å¤±è´¥: {e}")
            raise


class MemoryBandwidthOptimizer:
    """
    å†…å­˜å¸¦å®½ä¼˜åŒ–å™¨
    
    ä¼˜åŒ–GPUå†…å­˜è®¿é—®æ¨¡å¼ï¼Œæœ€å¤§åŒ–å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
    """
    
    def __init__(self, device: str):
        self.device = torch.device(device)
        self.logger = self._setup_logger()
        
        # å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
        self.tensor_cache: Dict[str, torch.Tensor] = {}
        self.memory_pool: List[torch.Tensor] = []
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger("MemoryBandwidthOptimizer")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [å†…å­˜å¸¦å®½ä¼˜åŒ–å™¨] %(levelname)s: %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def optimize_tensor_layout(self, tensor: torch.Tensor) -> torch.Tensor:
        """ä¼˜åŒ–å¼ é‡å†…å­˜å¸ƒå±€"""
        if not tensor.is_contiguous():
            tensor = tensor.contiguous()
        
        # å¦‚æœå¯èƒ½ï¼Œè½¬æ¢ä¸ºæ›´é«˜æ•ˆçš„æ•°æ®ç±»å‹
        if tensor.dtype == torch.float64:
            tensor = tensor.float()  # float64 -> float32
        
        return tensor
    
    def create_memory_pool(self, shapes: List[Tuple], pool_size: int = 100):
        """åˆ›å»ºå†…å­˜æ± ä»¥å‡å°‘åˆ†é…å¼€é”€"""
        self.logger.info(f"åˆ›å»ºå†…å­˜æ± : {len(shapes)} ç§å½¢çŠ¶, æ± å¤§å°: {pool_size}")
        
        for shape in shapes:
            for _ in range(pool_size // len(shapes)):
                tensor = torch.empty(shape, device=self.device, dtype=torch.float32)
                self.memory_pool.append(tensor)
        
        self.logger.info(f"å†…å­˜æ± åˆ›å»ºå®Œæˆï¼Œå…± {len(self.memory_pool)} ä¸ªå¼ é‡")
    
    def get_tensor_from_pool(self, shape: Tuple) -> Optional[torch.Tensor]:
        """ä»å†…å­˜æ± è·å–å¼ é‡"""
        for i, tensor in enumerate(self.memory_pool):
            if tensor.shape == shape:
                return self.memory_pool.pop(i)
        return None
    
    def return_tensor_to_pool(self, tensor: torch.Tensor):
        """å½’è¿˜å¼ é‡åˆ°å†…å­˜æ± """
        if len(self.memory_pool) < 200:  # é™åˆ¶æ± å¤§å°
            tensor.zero_()  # æ¸…é›¶ä½†ä¿ç•™å†…å­˜
            self.memory_pool.append(tensor)
    
    def optimize_memory_access_pattern(self, tensors: List[torch.Tensor]) -> List[torch.Tensor]:
        """ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼"""
        # æŒ‰å¤§å°æ’åºï¼Œä¼˜åŒ–è®¿é—®å±€éƒ¨æ€§
        tensors_with_size = [(t, t.numel()) for t in tensors]
        tensors_with_size.sort(key=lambda x: x[1])
        
        return [t[0] for t in tensors_with_size]


class AlgorithmEfficiencyOptimizer:
    """
    ç®—æ³•æ•ˆç‡ä¼˜åŒ–å™¨
    
    ä¼˜åŒ–æ ¸å¿ƒç®—æ³•ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ï¼Œæé«˜ç®—æ³•æ•ˆç‡
    """
    
    def __init__(self):
        self.logger = self._setup_logger()
        
        # è®¡ç®—ç¼“å­˜
        self.computation_cache: Dict[str, Any] = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger("AlgorithmEfficiencyOptimizer")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [ç®—æ³•æ•ˆç‡ä¼˜åŒ–å™¨] %(levelname)s: %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def optimize_model_forward(self, model: nn.Module, config: OptimizationConfig) -> nn.Module:
        """ä¼˜åŒ–æ¨¡å‹å‰å‘ä¼ æ’­"""
        self.logger.info("ğŸ”§ ä¼˜åŒ–æ¨¡å‹å‰å‘ä¼ æ’­")
        
        # 1. æ¨¡å‹ç¼–è¯‘ (PyTorch 2.0+)
        if config.compile_model and hasattr(torch, 'compile'):
            try:
                model = torch.compile(model, mode='max-autotune')
                self.logger.info("âœ… æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–å·²å¯ç”¨")
            except Exception as e:
                self.logger.warning(f"æ¨¡å‹ç¼–è¯‘å¤±è´¥: {e}")
        
        # 2. èåˆä¼˜åŒ–
        try:
            # èåˆBatchNormå’ŒConvå±‚
            torch.jit.optimize_for_inference(torch.jit.script(model))
            self.logger.info("âœ… å±‚èåˆä¼˜åŒ–å·²åº”ç”¨")
        except Exception as e:
            self.logger.warning(f"å±‚èåˆä¼˜åŒ–å¤±è´¥: {e}")
        
        # 3. æ¢¯åº¦æ£€æŸ¥ç‚¹ (è®­ç»ƒæ—¶)
        if config.gradient_checkpointing and hasattr(model, 'gradient_checkpointing_enable'):
            try:
                model.gradient_checkpointing_enable()
                self.logger.info("âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
            except Exception as e:
                self.logger.warning(f"æ¢¯åº¦æ£€æŸ¥ç‚¹å¯ç”¨å¤±è´¥: {e}")
        
        return model
    
    def cached_computation(self, key: str, computation_func: Callable, *args, **kwargs) -> Any:
        """å¸¦ç¼“å­˜çš„è®¡ç®—"""
        cache_key = f"{key}_{hash(str(args))}_{hash(str(sorted(kwargs.items())))}"
        
        if cache_key in self.computation_cache:
            self.cache_hits += 1
            return self.computation_cache[cache_key]
        
        self.cache_misses += 1
        result = computation_func(*args, **kwargs)
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.computation_cache) < 1000:
            self.computation_cache[cache_key] = result
        
        return result
    
    def get_cache_stats(self) -> Dict[str, float]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            'cache_hit_rate': hit_rate,
            'cache_size': len(self.computation_cache),
            'total_requests': total_requests
        }
    
    def optimize_inference_pipeline(self, 
                                  model: nn.Module, 
                                  preprocessing_func: Callable,
                                  postprocessing_func: Callable) -> Callable:
        """ä¼˜åŒ–æ¨ç†æµæ°´çº¿"""
        def optimized_pipeline(inputs):
            # 1. æ‰¹é‡é¢„å¤„ç†
            if isinstance(inputs, list):
                # æ‰¹é‡å¤„ç†å¤šä¸ªè¾“å…¥
                preprocessed = [preprocessing_func(inp) for inp in inputs]
                if preprocessed and isinstance(preprocessed[0], torch.Tensor):
                    # å°è¯•æ‰¹é‡åŒ–
                    try:
                        batch_input = torch.stack(preprocessed)
                        with torch.no_grad():
                            batch_output = model(batch_input)
                        
                        # åˆ†è§£æ‰¹é‡è¾“å‡º
                        outputs = [postprocessing_func(out) for out in batch_output]
                        return outputs
                    except Exception:
                        # å›é€€åˆ°å•ç‹¬å¤„ç†
                        pass
            
            # 2. å•ä¸ªæˆ–æ— æ³•æ‰¹é‡åŒ–çš„å¤„ç†
            if not isinstance(inputs, list):
                inputs = [inputs]
            
            outputs = []
            for inp in inputs:
                preprocessed = preprocessing_func(inp)
                with torch.no_grad():
                    output = model(preprocessed.unsqueeze(0) if preprocessed.dim() == 3 else preprocessed)
                result = postprocessing_func(output)
                outputs.append(result)
            
            return outputs[0] if len(outputs) == 1 else outputs
        
        return optimized_pipeline


class ComputeEfficiencyMaximizer:
    """
    è®¡ç®—æ•ˆç‡æœ€å¤§åŒ–ä¸»æ§åˆ¶å™¨
    
    æ•´åˆæ‰€æœ‰ä¼˜åŒ–ç­–ç•¥ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£å’Œè‡ªåŠ¨åŒ–ä¼˜åŒ–
    """
    
    def __init__(self, config: OptimizationConfig = None):
        self.config = config or OptimizationConfig()
        self.logger = self._setup_logger()
        
        # ä¼˜åŒ–å™¨ç»„ä»¶
        self.batch_optimizer = None
        self.parallel_engine = ParallelComputeEngine(self.config)
        self.memory_optimizer = None
        self.algorithm_optimizer = AlgorithmEfficiencyOptimizer()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_monitor = PerformanceMonitor()
        
        # ä¼˜åŒ–å†å²
        self.optimization_history: List[PerformanceMetrics] = []
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger("ComputeEfficiencyMaximizer")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [è®¡ç®—æ•ˆç‡æœ€å¤§åŒ–å™¨] %(levelname)s: %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def optimize_model(self, 
                      model: nn.Module, 
                      sample_input_shape: Tuple,
                      device: str = 'cuda:0') -> Tuple[nn.Module, int]:
        """
        å…¨é¢ä¼˜åŒ–æ¨¡å‹
        
        Args:
            model: å¾…ä¼˜åŒ–çš„æ¨¡å‹
            sample_input_shape: æ ·æœ¬è¾“å…¥å½¢çŠ¶
            device: ç›®æ ‡è®¾å¤‡
            
        Returns:
            (ä¼˜åŒ–åçš„æ¨¡å‹, æœ€ä¼˜æ‰¹æ¬¡å¤§å°)
        """
        self.logger.info("ğŸš€ å¼€å§‹å…¨é¢ä¼˜åŒ–æ¨¡å‹è®¡ç®—æ•ˆç‡")
        
        # 1. è®¾å¤‡å‡†å¤‡
        device = torch.device(device)
        model = model.to(device)
        
        # 2. åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.batch_optimizer = DynamicBatchOptimizer(device, model)
        self.memory_optimizer = MemoryBandwidthOptimizer(device)
        
        # 3. ç®—æ³•ä¼˜åŒ–
        self.logger.info("ğŸ”§ åº”ç”¨ç®—æ³•ä¼˜åŒ–")
        model = self.algorithm_optimizer.optimize_model_forward(model, self.config)
        
        # 4. å¹¶è¡Œä¼˜åŒ–
        if len(self.parallel_engine.available_gpus) > 1:
            self.logger.info("ğŸ”§ è®¾ç½®å¤šGPUå¹¶è¡Œ")
            model = self.parallel_engine.setup_data_parallel(model)
        
        # 5. æ··åˆç²¾åº¦ä¼˜åŒ–
        if self.config.mixed_precision and device.type == 'cuda':
            try:
                model = model.half()  # è½¬æ¢ä¸ºFP16
                self.logger.info("âœ… æ··åˆç²¾åº¦ä¼˜åŒ–å·²å¯ç”¨")
            except Exception as e:
                self.logger.warning(f"æ··åˆç²¾åº¦ä¼˜åŒ–å¤±è´¥: {e}")
                model = model.float()  # å›é€€åˆ°FP32
        
        # 6. å¯»æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å°
        self.logger.info("ğŸ” å¯»æ‰¾æœ€ä¼˜æ‰¹æ¬¡å¤§å°")
        optimal_batch_size = self.batch_optimizer.find_optimal_batch_size(
            sample_input_shape,
            max_iterations=15,
            target_utilization=0.85
        )
        
        # 7. å†…å­˜ä¼˜åŒ–
        if device.type == 'cuda':
            common_shapes = [
                (optimal_batch_size, *sample_input_shape),
                (optimal_batch_size // 2, *sample_input_shape),
                (optimal_batch_size * 2, *sample_input_shape),
            ]
            self.memory_optimizer.create_memory_pool(common_shapes)
        
        # 8. æ€§èƒ½éªŒè¯
        final_metrics = self._validate_optimization(model, sample_input_shape, optimal_batch_size, device)
        self.optimization_history.append(final_metrics)
        
        self.logger.info("âœ… æ¨¡å‹ä¼˜åŒ–å®Œæˆ")
        self.logger.info(f"   æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {optimal_batch_size}")
        self.logger.info(f"   æœ€ç»ˆååé‡: {final_metrics.throughput:.1f} samples/sec")
        self.logger.info(f"   GPUåˆ©ç”¨ç‡: {final_metrics.gpu_utilization:.1f}%")
        self.logger.info(f"   å†…å­˜æ•ˆç‡: {final_metrics.memory_efficiency:.1f}%")
        
        return model, optimal_batch_size
    
    def _validate_optimization(self, 
                              model: nn.Module, 
                              input_shape: Tuple, 
                              batch_size: int, 
                              device: torch.device) -> PerformanceMetrics:
        """éªŒè¯ä¼˜åŒ–æ•ˆæœ"""
        self.logger.info("ğŸ“Š éªŒè¯ä¼˜åŒ–æ•ˆæœ")
        
        try:
            model.eval()
            test_data = torch.randn(batch_size, *input_shape, device=device)
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(5):
                    _ = model(test_data)
            
            torch.cuda.synchronize(device) if device.type == 'cuda' else None
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            test_steps = 20
            
            with torch.no_grad():
                for _ in range(test_steps):
                    _ = model(test_data)
            
            torch.cuda.synchronize(device) if device.type == 'cuda' else None
            end_time = time.time()
            
            # è®¡ç®—æŒ‡æ ‡
            total_samples = batch_size * test_steps
            total_time = end_time - start_time
            
            metrics = PerformanceMetrics()
            metrics.throughput = total_samples / total_time
            metrics.latency = (total_time / test_steps) * 1000
            
            if device.type == 'cuda':
                memory_info = torch.cuda.memory_stats(device)
                allocated = memory_info.get('allocated_bytes.all.current', 0)
                reserved = memory_info.get('reserved_bytes.all.current', 0)
                total_memory = torch.cuda.get_device_properties(device).total_memory
                
                metrics.memory_efficiency = (allocated / total_memory) * 100
                
                # GPUåˆ©ç”¨ç‡ä¼°ç®—
                theoretical_flops = self._estimate_model_flops(model, input_shape)
                actual_flops = theoretical_flops * (total_samples / total_time)
                peak_flops = self._estimate_peak_flops(device)
                metrics.compute_efficiency = min(100.0, (actual_flops / peak_flops) * 100)
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            metrics.recommendations = self._generate_recommendations(metrics)
            
            del test_data
            return metrics
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½éªŒè¯å¤±è´¥: {e}")
            return PerformanceMetrics()
    
    def _estimate_model_flops(self, model: nn.Module, input_shape: Tuple) -> float:
        """ä¼°ç®—æ¨¡å‹FLOPS"""
        # ç®€åŒ–çš„FLOPSä¼°ç®—
        total_params = sum(p.numel() for p in model.parameters())
        # ç²—ç•¥ä¼°è®¡ï¼šæ¯ä¸ªå‚æ•°å¤§çº¦éœ€è¦2ä¸ªFLOPS (ä¹˜æ³•+åŠ æ³•)
        return total_params * 2.0
    
    def _estimate_peak_flops(self, device: torch.device) -> float:
        """ä¼°ç®—è®¾å¤‡å³°å€¼FLOPS"""
        if device.type == 'cuda':
            try:
                props = torch.cuda.get_device_properties(device)
                # ç®€åŒ–ä¼°ç®—ï¼šåŸºäºSMæ•°é‡å’Œé¢‘ç‡
                base_flops = props.multi_processor_count * 1000 * 1000 * 1000  # 1 GFLOPS per SM
                return base_flops
            except:
                return 10 * 1000 * 1000 * 1000  # é»˜è®¤10 GFLOPS
        else:
            return 100 * 1000 * 1000  # CPUå¤§çº¦100 MFLOPS
    
    def _generate_recommendations(self, metrics: PerformanceMetrics) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if metrics.memory_efficiency < 50:
            recommendations.append("å†…å­˜åˆ©ç”¨ç‡è¾ƒä½ï¼Œè€ƒè™‘å¢åŠ æ‰¹æ¬¡å¤§å°")
        elif metrics.memory_efficiency > 90:
            recommendations.append("å†…å­˜ä½¿ç”¨ç‡å¾ˆé«˜ï¼Œæ³¨æ„å†…å­˜æº¢å‡ºé£é™©")
        
        if metrics.compute_efficiency < 30:
            recommendations.append("è®¡ç®—æ•ˆç‡è¾ƒä½ï¼Œæ£€æŸ¥æ¨¡å‹å¤æ‚åº¦å’Œæ•°æ®ä¼ è¾“")
        
        if metrics.throughput < 100:
            recommendations.append("ååé‡è¾ƒä½ï¼Œè€ƒè™‘å¯ç”¨æ··åˆç²¾åº¦æˆ–æ¨¡å‹ä¼˜åŒ–")
        
        # ç¼“å­˜ç»Ÿè®¡å»ºè®®
        cache_stats = self.algorithm_optimizer.get_cache_stats()
        if cache_stats['cache_hit_rate'] < 0.5:
            recommendations.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œè€ƒè™‘ä¼˜åŒ–è®¡ç®—æ¨¡å¼")
        
        if not recommendations:
            recommendations.append("æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå·²å……åˆ†ä¼˜åŒ–")
        
        return recommendations
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
        if not self.optimization_history:
            return {"error": "æ²¡æœ‰ä¼˜åŒ–å†å²è®°å½•"}
        
        latest_metrics = self.optimization_history[-1]
        cache_stats = self.algorithm_optimizer.get_cache_stats()
        
        return {
            "latest_performance": {
                "throughput": latest_metrics.throughput,
                "latency": latest_metrics.latency,
                "gpu_utilization": latest_metrics.gpu_utilization,
                "memory_efficiency": latest_metrics.memory_efficiency,
                "compute_efficiency": latest_metrics.compute_efficiency
            },
            "optimization_config": {
                "optimization_level": self.config.optimization_level.value,
                "mixed_precision": self.config.mixed_precision,
                "tensor_parallel": self.config.tensor_parallel,
                "max_batch_size": self.config.max_batch_size
            },
            "cache_performance": cache_stats,
            "recommendations": latest_metrics.recommendations,
            "hardware_info": {
                "available_gpus": len(self.parallel_engine.available_gpus),
                "cpu_cores": self.parallel_engine.cpu_cores
            }
        }


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.monitoring_active = False
        self.monitor_thread = None
        
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—å™¨"""
        logger = logging.getLogger("PerformanceMonitor")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '[%(asctime)s] [æ€§èƒ½ç›‘æ§å™¨] %(levelname)s: %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def start_monitoring(self, interval: float = 5.0):
        """å¼€å§‹æ€§èƒ½ç›‘æ§"""
        if self.monitoring_active:
            return
        
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_loop,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        self.logger.info(f"æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ (é—´éš”: {interval}s)")
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.monitoring_active = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=2.0)
        self.logger.info("æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self, interval: float):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring_active:
            try:
                # ç›‘æ§GPUçŠ¶æ€
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        memory_info = torch.cuda.memory_stats(i)
                        allocated = memory_info.get('allocated_bytes.all.current', 0) / (1024**3)
                        reserved = memory_info.get('reserved_bytes.all.current', 0) / (1024**3)
                        
                        if allocated > 0:  # åªè®°å½•æœ‰ä½¿ç”¨çš„GPU
                            self.logger.info(f"GPU {i}: å·²åˆ†é… {allocated:.1f} GB, å·²ä¿ç•™ {reserved:.1f} GB")
                
                time.sleep(interval)
                
            except Exception as e:
                self.logger.error(f"æ€§èƒ½ç›‘æ§å‡ºé”™: {e}")
                time.sleep(interval)


# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
_global_efficiency_maximizer = None

def get_efficiency_maximizer(config: OptimizationConfig = None) -> ComputeEfficiencyMaximizer:
    """è·å–å…¨å±€è®¡ç®—æ•ˆç‡æœ€å¤§åŒ–å™¨"""
    global _global_efficiency_maximizer
    if _global_efficiency_maximizer is None:
        _global_efficiency_maximizer = ComputeEfficiencyMaximizer(config)
    return _global_efficiency_maximizer


def optimize_model_for_maximum_efficiency(model: nn.Module, 
                                        sample_input_shape: Tuple,
                                        device: str = 'cuda:0',
                                        optimization_level: ComputeOptimizationLevel = ComputeOptimizationLevel.BALANCED) -> Tuple[nn.Module, int]:
    """
    ä¾¿æ·å‡½æ•°ï¼šä¸ºæ¨¡å‹åº”ç”¨æœ€å¤§åŒ–è®¡ç®—æ•ˆç‡ä¼˜åŒ–
    
    Args:
        model: å¾…ä¼˜åŒ–çš„æ¨¡å‹
        sample_input_shape: æ ·æœ¬è¾“å…¥å½¢çŠ¶
        device: ç›®æ ‡è®¾å¤‡
        optimization_level: ä¼˜åŒ–çº§åˆ«
        
    Returns:
        (ä¼˜åŒ–åçš„æ¨¡å‹, æœ€ä¼˜æ‰¹æ¬¡å¤§å°)
    """
    config = OptimizationConfig(optimization_level=optimization_level)
    maximizer = get_efficiency_maximizer(config)
    return maximizer.optimize_model(model, sample_input_shape, device)


if __name__ == "__main__":
    # æµ‹è¯•è®¡ç®—æ•ˆç‡ä¼˜åŒ–
    print("ğŸš€ æµ‹è¯•è®¡ç®—æ•ˆç‡æœ€å¤§åŒ–ä¼˜åŒ–å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    test_model = nn.Sequential(
        nn.Linear(64, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 10)
    )
    
    # ä¼˜åŒ–æ¨¡å‹
    if torch.cuda.is_available():
        optimized_model, optimal_batch = optimize_model_for_maximum_efficiency(
            test_model, 
            (64,),  # è¾“å…¥å½¢çŠ¶
            'cuda:0',
            ComputeOptimizationLevel.AGGRESSIVE
        )
        
        print(f"âœ… ä¼˜åŒ–å®Œæˆï¼æœ€ä¼˜æ‰¹æ¬¡å¤§å°: {optimal_batch}")
        
        # è·å–ä¼˜åŒ–æŠ¥å‘Š
        maximizer = get_efficiency_maximizer()
        report = maximizer.get_optimization_report()
        print(f"ğŸ“Š ä¼˜åŒ–æŠ¥å‘Š: {report}")
    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUä¼˜åŒ–æµ‹è¯•")
    
    print("æµ‹è¯•å®Œæˆ")